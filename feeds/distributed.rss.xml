<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>particle128's Blog</title><link href="http://particle128.com/" rel="alternate"></link><link href="http://particle128.com/feeds/distributed.rss.xml" rel="self"></link><id>http://particle128.com/</id><updated>2014-08-25T00:00:00+08:00</updated><entry><title>brief introduction of storm</title><link href="http://particle128.com/posts/2014/08/introduction-storm.html" rel="alternate"></link><updated>2014-08-25T00:00:00+08:00</updated><author><name>particle128</name></author><id>tag:particle128.com,2014-08-25:posts/2014/08/introduction-storm.html</id><summary type="html">&lt;p&gt;I have used Storm for 3 months at my internship in Alibaba, and this article is a brief introduction of storm. Use English for fun, haha...&lt;/p&gt;
&lt;h3&gt;What is storm&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://storm.incubator.apache.org/"&gt;Storm&lt;/a&gt; is a distributed realtime computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing realtime computation.&lt;br /&gt;
Many large companies, including Twitter, Yahoo!, Alibaba and Baidu, have used Storm to do some realtime processing, such as realtime analytics of application logs, or DRPC. Twitter integrates storm with its other infrastructures, including database systems, the messaging infrastructure, and monitoring/altering system.&lt;br /&gt;
Typical use cases for Storm:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;processing stream&lt;br /&gt;
  no need for intermediary queues -&amp;gt; faster  &lt;/li&gt;
&lt;li&gt;continuous computation&lt;br /&gt;
  compute when data is available -&amp;gt; realtime  &lt;/li&gt;
&lt;li&gt;distributed remote procedure call&lt;br /&gt;
  parallelize CPU-intensive operations -&amp;gt; higher throughput (somewhat like pipelining)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By the way, Alibaba has developed a special version of Storm, called &lt;a href="https://github.com/alibaba/jstorm"&gt;JStorm&lt;/a&gt;, which ueses Java (quicker than Clojure) to rewrite the core module of Storm and makes some changes (optimizing ack mechanism, ...), achieving higher performance and more stability.  &lt;/p&gt;
&lt;h3&gt;Basic concepts of storm&lt;/h3&gt;
&lt;h4&gt;Stream&lt;/h4&gt;
&lt;p&gt;A &lt;code&gt;stream&lt;/code&gt; is an unbounded sequence of data that is created and processed in parallel in a distributed fashion.&lt;br /&gt;
Streams are composed of &lt;code&gt;tuples&lt;/code&gt;, which are the smallest units that can be emited from spout/bolt. And a tuple is constuited of 1 or more &lt;code&gt;fields&lt;/code&gt;.&lt;br /&gt;
For example, &lt;code&gt;TestSpout&lt;/code&gt; reads tweets from a MQ, and emits &lt;code&gt;(author, tweet, date)&lt;/code&gt; to &lt;code&gt;TestBolt&lt;/code&gt;. &lt;code&gt;(author, tweet, date)&lt;/code&gt; is the tuple, and &lt;code&gt;author&lt;/code&gt;, &lt;code&gt;tweet&lt;/code&gt;, &lt;code&gt;date&lt;/code&gt; are three fields of this tuple.  &lt;/p&gt;
&lt;h4&gt;Spout &amp;amp; Bolt&lt;/h4&gt;
&lt;p&gt;The input stream of a Storm cluster is handled by a component called a &lt;code&gt;spout&lt;/code&gt;. The spout passes the data to a component called a &lt;code&gt;bolt&lt;/code&gt;, which transforms it in some way. A bolt either persists the data in some sort of storage, or passes it to some other bolt. You can imagine a Storm cluster as a chain of bolt components that each make some kind of transformation on the data exposed by the spout.  &lt;/p&gt;
&lt;h4&gt;Connection of spout/bolt&lt;/h4&gt;
&lt;p&gt;there are 8 grouping methods, which defines how the downstream bolt read from the upstream spout/bolt.&lt;br /&gt;
Common grouping method includes  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shuffle grouping&lt;br /&gt;
  Tuples are randomly distributed across the bolt’s tasks in a way such that each bolt is guaranteed to get an equal number of tuples.  &lt;/li&gt;
&lt;li&gt;Fields grouping&lt;br /&gt;
  The stream is partitioned by the fields specified in the grouping. For example, if the stream is grouped by the “user-id” field, tuples with the same “user-id” will always go to the same task, but tuples with different “user-id”’s may go to different tasks.  &lt;/li&gt;
&lt;li&gt;All grouping&lt;br /&gt;
  The stream is replicated across all the bolt’s tasks. Use this grouping with care.  &lt;/li&gt;
&lt;li&gt;Global grouping&lt;br /&gt;
  The entire stream goes to a single one of the bolt’s tasks. Specifically, it goes to the task with the lowest id.  &lt;/li&gt;
&lt;li&gt;Custom grouping&lt;br /&gt;
  You defines the grouping rule.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Topology&lt;/h4&gt;
&lt;p&gt;The arrangement of all the components (spouts and bolts) and their connections is called a &lt;code&gt;topology&lt;/code&gt;  &lt;/p&gt;
&lt;h4&gt;Nimbus &amp;amp; Supervior&lt;/h4&gt;
&lt;p&gt;Since it's a distributed system, there are more than one machine (node) in the system's cluster.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;master node&lt;br /&gt;
  runs &lt;code&gt;Nimbus&lt;/code&gt; daemon, which is responsible for distributing code around the cluster, assigning tasks to each worker node, and monitoring for failures.  &lt;/li&gt;
&lt;li&gt;worker node&lt;br /&gt;
  runs &lt;code&gt;Supervisor&lt;/code&gt; daemon, which consists of 1 or more worker processes and executes a portion of a topology (such as 1 spout and 2 bolts)  &lt;/li&gt;
&lt;li&gt;zookeeper node&lt;br /&gt;
  runs zookeeper, in which the states of Nimbus and Supervisor are stored  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The illustration below shows the relationship among them&lt;br /&gt;
&lt;img alt="image" src="http://particle128.com/images/storm1.png" /&gt;  &lt;/p&gt;
&lt;h4&gt;worker &amp;amp; executor &amp;amp; task&lt;/h4&gt;
&lt;p&gt;all the three concepts are inside the supervisor daemon, since the master node and the zk node are controlling nodes  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;worker (process)&lt;br /&gt;
  runs inside the supervisor daemon&lt;br /&gt;
  You can configure the number of workers across machines by &lt;code&gt;Config config = new Config(); config.setNumWorkers(3)&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;executor (thread)&lt;br /&gt;
  runs inside the worker process&lt;br /&gt;
  You can configure the initial number of executors per component(bolt/spout) by &lt;code&gt;topologyBuilder.setBolt("test", new TestBolt(), 2)&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;task (bolt/spout instance)&lt;br /&gt;
  runs inside the executor, 1 executor has 1 task by default&lt;br /&gt;
  You can configure the number of tasks per component by &lt;code&gt;topologyBuilder.setBolt("test", new TestBolt(), 2).setNumTasks(4)&lt;/code&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following two illustrations show the relationship among them&lt;br /&gt;
&lt;img alt="image" src="http://particle128.com/images/storm2.png" /&gt;&lt;br /&gt;
&lt;img alt="image" src="http://particle128.com/images/storm3.png" /&gt;  &lt;/p&gt;
&lt;p&gt;What is the reason for have 2+ tasks per executor, see &lt;a href="http://stackoverflow.com/questions/17257448/what-is-the-task-in-twitter-storm-parallelism"&gt;here&lt;/a&gt;  &lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://storm.incubator.apache.org/documentation"&gt;storm documentation&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://www.michael-noll.com/blog/2012/10/16/understanding-the-parallelism-of-a-storm-topology/"&gt;Understanding the Parallelism of a Storm Topology&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://stackoverflow.com/questions/17257448/what-is-the-task-in-twitter-storm-parallelism"&gt;What is the “task” in twitter Storm parallelism&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://shop.oreilly.com/product/0636920024835.do"&gt;Getting Started with Storm&lt;/a&gt;  &lt;/p&gt;</summary><category term="storm"></category><category term="trident"></category></entry><entry><title>使用Storm 2个月后的总结</title><link href="http://particle128.com/posts/2014/07/storm-sum.html" rel="alternate"></link><updated>2014-07-29T00:00:00+08:00</updated><author><name>particle128</name></author><id>tag:particle128.com,2014-07-29:posts/2014/07/storm-sum.html</id><summary type="html">&lt;h2&gt;如何使用directGrouping&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;申明流是direct Stream&lt;br /&gt;
spout/bolt的declareOutputFields中&lt;br /&gt;
outputFieldsDeclarer.declare(/&lt;em&gt;direct&lt;/em&gt;/true, new Fields("word"));  &lt;/li&gt;
&lt;li&gt;使用emitDirect来发送数据&lt;br /&gt;
spout/bolt的nextTuple/execute中&lt;br /&gt;
collector.emitDirect(/&lt;em&gt;taskId&lt;/em&gt;/getWordCountIndex(word),new Values(word));&lt;br /&gt;
注意：因为第一个参数是taskid，首先需要在open/prepare里面获取下游bolt的taskid列表&lt;br /&gt;
topologyContext.getComponentTasks("word-counter")  &lt;/li&gt;
&lt;li&gt;topology定义中指定连接方式为directGrouping&lt;br /&gt;
builder.setBolt("word-counter", new WordCounter(), 3).directGrouping("spout");  &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;如何利用Storm提供的可靠性保证&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;在spout中，用SpoutOutputCollector调用emit的时候，必须指定messageId。&lt;br /&gt;
一个messageId对应一个tuple树。&lt;br /&gt;
这样，在某个tuple树全部被处理完后，spout的回调函数ack会被调用，ack的参数是那个tuple树对应的messageId。  &lt;/li&gt;
&lt;li&gt;在每一个bolt中，用OutputCollector调用emit的时候，需要指定源tuple，并且在emit之后要调用ack。&lt;br /&gt;
如果该bolt继承自BaseBasicBolt，则自动完成上述操作。  &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;问题集锦&lt;/h2&gt;
&lt;h3&gt;Problem1&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;br /&gt;
Spout：emit出去一个map，下游Bolt：有时候读到空的map，有时候读到有内容的map&lt;br /&gt;
&lt;strong&gt;原因&lt;/strong&gt;&lt;br /&gt;
emit出去一个map，就调用pruneData函数把map给clear了。&lt;br /&gt;
因为本地模式在同一个虚拟机下，storm并没有把这个map深拷贝到tuple中保存，Bolt读到的map和上游Spout发送的map是来自同一块内存。&lt;br /&gt;
这样，就出现了一个竞争条件：如果Spout线程先clear，下游Bolt才接到的话，读到的就是空map；如果先接到，Spout再clear，读到的就是有内容的map。&lt;br /&gt;
&lt;strong&gt;解决&lt;/strong&gt;&lt;br /&gt;
pruneData不clear这个map了，而是new一个新的map。让垃圾回收器去把不用的map回收。  &lt;/p&gt;
&lt;h3&gt;Problem2&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;br /&gt;
Trident中，使用each和partitionAggregate函数，下游接收到空的tuple&lt;br /&gt;
&lt;strong&gt;原因&lt;/strong&gt;&lt;br /&gt;
这和&lt;a href="http://storm.incubator.apache.org/documentation/Documentation.html"&gt;官方Documentation&lt;/a&gt;不符，原因不详&lt;br /&gt;
&lt;strong&gt;解决&lt;/strong&gt;&lt;br /&gt;
不管是each，还是partitionAggregate函数，必须指定第一个参数（即Fields），表明输入的值列表。  &lt;/p&gt;
&lt;h3&gt;Probelm3&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;br /&gt;
Trident中，一旦加上partitionPersist函数，就会报错  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;NotSerializableException&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;aliyun&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;aep&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;storm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;binpacking&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;BinpackingTrident&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;解决&lt;/strong&gt;&lt;br /&gt;
让BinpackTrident实现Serializable。  &lt;/p&gt;</summary><category term="storm"></category></entry><entry><title>大规模分布式存储系统-笔记（2）</title><link href="http://particle128.com/posts/2014/04/distriStore2.html" rel="alternate"></link><updated>2014-04-26T00:00:00+08:00</updated><author><name>particle128</name></author><id>tag:particle128.com,2014-04-26:posts/2014/04/distriStore2.html</id><summary type="html">&lt;h2&gt;Chapter2 单机存储系统&lt;/h2&gt;
&lt;h3&gt;2.1硬件基础&lt;/h3&gt;
&lt;h4&gt;CPU架构&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;SMP（对称多处理）:经典的CPU架构，一个计算机上汇集了一组CPU（一个CPU往往包括不止一个核），对称工作、无主次或从属关系，共享相同的物理内存和总线。&lt;br /&gt;
  某些CPU还通过超线程技术（Hyper-Threading Technology），在一个核心上同时执行两个线程。  &lt;/li&gt;
&lt;li&gt;NUMA（非一致存储访问）:克服SMP中多CPU对总线资源的竞争，一个NUMA包括多个NUMA节点，一个NUMA节点就是一个SMP结构（n个CPU，1个内存）  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;IO总线&lt;/h4&gt;
&lt;p&gt;Intel的主板，一般为南、北桥架构  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;北桥芯片：CPU，内存，高端SSD (高速IO)  &lt;/li&gt;
&lt;li&gt;南桥芯片：网卡，硬盘，中低端SSD  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;网路拓扑&lt;/h4&gt;
&lt;p&gt;数据中心网络拓扑结构，主要分为  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;三层结构：核心层、汇聚层、接入层。一个接入层交换机下的服务器部署在一个机架内  &lt;/li&gt;
&lt;li&gt;三级CLOS网络：Google设计的扁平化拓扑结构（从wiki上看图，感觉挺复杂的）  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;传输时延  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同一个数据中心内部，网络来回的时延为0.5ms  &lt;/li&gt;
&lt;li&gt;杭州和北京两个数据数据中心之间，网络来回的时延大约40ms  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;性能参数&lt;/h4&gt;
&lt;p&gt;比较重要的参数  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;内存访问：               100ns  &lt;/li&gt;
&lt;li&gt;千兆网络发送1MB数据：    10ms  (1Gbit/s=0.1GB/s=100MB/s，1MB/(100MB/1000ms)=10ms)  &lt;/li&gt;
&lt;li&gt;SATA磁盘寻道：           10ms  &lt;/li&gt;
&lt;li&gt;SATA磁盘顺序读取1MB数据：20ms （SATA带宽100MB/s，寻道+访问=10ms+1MB/(100MB/1000ms)=20ms）  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外：&lt;br /&gt;
SSD（固态硬盘）因为随机读取延迟小（磁盘的随机读取，需要耗费大量的寻道时间），并且提供很高的IOPS（每秒读写次数），在缓存和一些性能要求较高的关键业务上的应用越来越广。  &lt;/p&gt;
&lt;p&gt;存储系统的性能包括两个维度 : 吞吐量 , 访问延时&lt;br /&gt;
磁盘和SSD的访问延时差别很大，但是吞吐量差别不大。  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;磁盘：大块顺序访问的存储系统，保存冷数据  &lt;/li&gt;
&lt;li&gt;SSD：随机访问较多或者对延时比较敏感的关键系统，保存热数据  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.2 单机存储引擎&lt;/h3&gt;
&lt;p&gt;存储引擎相当于存储系统的发动机.&lt;br /&gt;
存储引擎就是哈希表、B树、LSM树等数据结构在磁盘、SSD等持久化介质的实现。根据采用数据结构不同，分成：哈希存储引擎，B树存储引擎，LSM树存储引擎  &lt;/p&gt;
&lt;h4&gt;哈希存储引擎&lt;/h4&gt;
&lt;p&gt;Bitcask是&lt;a href="http://docs.basho.com/riak/1.2.0/tutorials"&gt;Riak&lt;/a&gt;（排名第三的键值数据库key-value store，elang实现，前两名是redis和memcached）的默认后台存储引擎。  &lt;/p&gt;
&lt;p&gt;特点：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只支持追加操作，磁盘中的数据文件包括active data file和older data file。当active data file达到一定大小，就会变成older data file，并且建立新的active文件用来追加数据。  &lt;/li&gt;
&lt;li&gt;磁盘中数据的格式：&lt;code&gt;crc,timestamp,key_sz,value_sz,key(变长),value(变长)&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;内存中的数据结构：&lt;code&gt;key&lt;/code&gt; -&amp;gt; &lt;code&gt;file_id,value_sz,value_pos,timestamp&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;定期合并：删除和更新后，原来的记录会变成垃圾数据，这时候需要合并older data file。基于时间戳，把同一个键下的旧记录或设置为无效的记录删除，只保留最新的记录。  &lt;/li&gt;
&lt;li&gt;快速恢复：内存中的哈希表，每次都在磁盘上保存一份，叫索引文件（hint file）。  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;操作：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;增：先在磁盘中追加一条记录，再更新内存的哈希表  &lt;/li&gt;
&lt;li&gt;删：更新磁盘中对应记录的value为无效，更新内存  &lt;/li&gt;
&lt;li&gt;改：增加一条新的记录，更新内存  &lt;/li&gt;
&lt;li&gt;查：通过内存哈希表找到文件号、value位置、value长度，直接read指定的文件在指定的偏移多少字节即可。  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;缺点：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有键都保存在内存中，需要保证键的范围不够大。  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;B树存储引擎&lt;/h4&gt;
&lt;p&gt;MySQL InnoDB是&lt;a href="http://dev.mysql.com/"&gt;MySQL&lt;/a&gt;(排名第二的数据库，第一是oracle)的一个存储引擎  &lt;/p&gt;
&lt;p&gt;缓冲区的管理方式：LIRS&lt;br /&gt;
是LRU替换算法的一种改进，被用在MySQL InnoDB和Oracle Touch Count中。将缓冲池分为两级，数据首先进入第一级，如果数据在较短时间内被访问两次或以上，则成为热点数据进入第二级，每一级内部还是采用LRU替换算法。&lt;br /&gt;
目的：防止之前的热点数据，被一次数据库遍历操作全部剔除出缓冲区。  &lt;/p&gt;
&lt;h4&gt;LSM树存储引擎&lt;/h4&gt;
&lt;p&gt;LSM树，即log structured merge tree， LevelDB采用。PS:erlang版本的eLevelDB是Riak的可选后台存储引擎。  &lt;/p&gt;
&lt;p&gt;特点：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对数据的修改增量保存在内存的MemTable中，因此修改操作比较快。  &lt;/li&gt;
&lt;li&gt;当MemTable大小达到上限，LevelDB将其冻结为“不可变MemTable”，即只可读的MemTable；同时重新生成一个MemTable，用于接收到达的写入请求。  &lt;/li&gt;
&lt;li&gt;写入操作：先写到磁盘中的操作日志，再更新内存中的MemTable。  &lt;/li&gt;
&lt;li&gt;合并操作(minor compaction)：后台线程将不可变MemTable中的数据排序后写入磁盘，形成SSTable文件。SSTable文件中的记录按照主键排序，每个文件有最小和最大的主键值，这些数据元信息保存于“清单文件”中。SSTable文件根据写入时间，分成多个层次，新写入的放入0层，旧的在1层，更旧的在2层，依次类推。  &lt;/li&gt;
&lt;li&gt;合并操作(major compaction)：当某个层级下的SSTable文件数目超过上限，合并该层中的某些文件到高层级中。  &lt;/li&gt;
&lt;li&gt;读取操作（LevelDB只支持随机读取单条记录）：按如下顺序读取，（1）内存中MemTable，（2）内存中的不可变MemTable，（3）从新到老读取磁盘中的SSTable文件。  &lt;/li&gt;
&lt;li&gt;当内存没有命中时，读取操作比较费时，需要在内存和各个层次文件中查找。  &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;小结（by myself）&lt;/h4&gt;
&lt;p&gt;几乎存储引擎都会先更新磁盘中的log，再写入内存。
Q:为什么不直接写入磁盘中的数据文件，再写入内存，都是一次磁盘io和一次内存访问？
A:我感觉，因为log文件总是需要的（转储时，数据库失败恢复时），所以必须要写log，而只写log不写磁盘中的数据文件，不会有任何影响，所以减少一次磁盘访问，就只写log和内存。&lt;/p&gt;
&lt;h3&gt;2.3数据模型&lt;/h3&gt;
&lt;p&gt;数据模型相当于存储系统的外壳.&lt;br /&gt;
包括:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文件模型，比如unix树状的文件系统，GFS，TFS等  &lt;/li&gt;
&lt;li&gt;关系模型，比如关系型数据库oracle,mysql  &lt;/li&gt;
&lt;li&gt;键值模型，比如redis  &lt;/li&gt;
&lt;li&gt;表格模型，比如Google Bigtable，HBase  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NoSQL(键值模型或表格模型)与SQL(关系模型)对比  &lt;/p&gt;
&lt;p&gt;在海量数据的场景中，SQL存在缺点：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;事务：事务的多个操作可能在不同主机上，为了保证原子性，需要“两阶段提交协议”（two-phase commit protocol,2PC）。该协议性能很低，而且不能容忍服务器故障。  &lt;/li&gt;
&lt;li&gt;多表连接：关系数据库讲究范式，为了达到更高级别的范式，需要拆分表格，这可能导致多个表在不同主机上，连接性能比较低。  &lt;/li&gt;
&lt;li&gt;性能：B树存储引擎，在某些特定的情形下不如LSM树和hash表。  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然，也有其优势：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标准：SQL语言标准化工作很好  &lt;/li&gt;
&lt;li&gt;运维：关系数据库有成熟的运维工具和大量运维人员  &lt;/li&gt;
&lt;/ul&gt;</summary><category term="distributed"></category><category term="storage"></category></entry><entry><title>大规模分布式存储系统-笔记（1）</title><link href="http://particle128.com/posts/2014/04/distriStore1.html" rel="alternate"></link><updated>2014-04-15T00:00:00+08:00</updated><author><name>particle128</name></author><id>tag:particle128.com,2014-04-15:posts/2014/04/distriStore1.html</id><summary type="html">&lt;h2&gt;Chapter1 概述&lt;/h2&gt;
&lt;h3&gt;分布式存储特征&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;可扩展：随着集群规模增大，整体性能表现为线性增长  &lt;/li&gt;
&lt;li&gt;低成本：构建在普通PC上，实现自动容错、自动负载均衡  &lt;/li&gt;
&lt;li&gt;高性能  &lt;/li&gt;
&lt;li&gt;易用：提供易用的外部接口，也具备完善的监控和运维工具  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;鸡汤：  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一般来讲，工程师如果能够深入理解分布式存储系统，理解其他互联网后台架构不再会有任何困难。  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;分布式存储分类&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;非结构化数据：文档、图片、视频等  &lt;/li&gt;
&lt;li&gt;结构化数据：数据库中的表  &lt;/li&gt;
&lt;li&gt;半结构化数据：HTML文档，模式结构和内容混在一起  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;分布式存储系统分类&lt;/h3&gt;
&lt;p&gt;1、分布式文件系统&lt;br /&gt;
存储非结构化数据，可以用作分布式数据库或分布式表格系统的底层存储  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;存储BLOB，Facebook Haystack以及Taobao File System  &lt;/li&gt;
&lt;li&gt;存储大文件，Google File System，分布式表格系统Google Bigtable的基础  &lt;/li&gt;
&lt;li&gt;存储定长块，Amazon的Elastic Block Store，分布式数据库Amazon RDS的基础  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2、分布式键值系统&lt;br /&gt;
存储关系简单的半结构化数据，只提供基于主键的CRUD功能，一般用作分布式缓存  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Dynamo  &lt;/li&gt;
&lt;li&gt;Taobao Tair  &lt;/li&gt;
&lt;li&gt;redis  &lt;/li&gt;
&lt;li&gt;memcached  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一致性哈希(consistent hashing)是分布式缓存常用的技术&lt;/p&gt;
&lt;p&gt;3、分布式表格系统&lt;br /&gt;
存储关系较为复杂的半结构化数据，除了CRUD功能，还支持主键的范围查找  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google Bigtable  &lt;/li&gt;
&lt;li&gt;Amazon DynamoDB  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4、分布式数据库&lt;br /&gt;
存储结构化数据，是关系型数据库的扩展，提供SQL关系查询语言，多表关联、嵌套子查询等。  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google Spanner  &lt;/li&gt;
&lt;li&gt;阿里的OceanBase  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然还有很火的NoSQL数据库，克服关系数据库扩展、并发、性能方面的缺陷。  &lt;/p&gt;</summary><category term="distributed"></category><category term="storage"></category></entry></feed>